#!/usr/bin/env python3
"""
Comprehensive Data Fetcher for Dylan Steman Content Generator
Integrates multiple free APIs: FRED, Census Bureau, BLS, and Zillow Research Data
"""

import os
import requests
# import pandas as pd  # Removed to avoid deployment issues
from datetime import datetime, timedelta
import json
from dotenv import load_dotenv
import time

# Load environment variables
load_dotenv()

class ComprehensiveDataFetcher:
    def __init__(self):
        self.fred_api_key = os.getenv('FRED_API_KEY')
        self.data_cache = {}
        
    def get_fred_data(self, series_list):
        """Fetch data from FRED API"""
        print("ğŸ“Š Fetching FRED economic data...")
        fred_data = {}
        
        for series_id, series_name in series_list:
            url = "https://api.stlouisfed.org/fred/series/observations"
            params = {
                'series_id': series_id,
                'api_key': self.fred_api_key,
                'file_type': 'json',
                'limit': 1,
                'sort_order': 'desc'
            }
            
            try:
                response = requests.get(url, params=params, timeout=10)
                if response.status_code == 200:
                    data = response.json()
                    if 'observations' in data and len(data['observations']) > 0:
                        latest = data['observations'][0]
                        fred_data[series_id] = {
                            'name': series_name,
                            'value': latest['value'],
                            'date': latest['date'],
                            'source': 'FRED'
                        }
                        print(f"âœ… {series_name}: {latest['value']} (as of {latest['date']})")
                    else:
                        print(f"âš ï¸  {series_name}: No data available")
                else:
                    print(f"âŒ {series_name}: API call failed (status: {response.status_code})")
                
                # Rate limiting - be respectful to APIs
                time.sleep(0.1)
                
            except Exception as e:
                print(f"âŒ Error fetching {series_name}: {e}")
        
        return fred_data
    
    def get_census_data(self):
        """Fetch data from Census Bureau API"""
        print("\nğŸ˜ï¸  Fetching Census Bureau data...")
        census_data = {}
        
        # American Community Survey - Key demographics for major metros
        # Using 2022 5-Year estimates (most recent comprehensive data)
        metros = {
            '16980': 'Chicago-Naperville-Elgin, IL-IN-WI',
            '33460': 'Minneapolis-St. Paul-Bloomington, MN-WI',
            '14460': 'Boston-Cambridge-Newton, MA-NHME',
            '33100': 'Miami-Fort Lauderdale-West Palm Beach, FL'
        }
        
        # Key variables for CRE analysis
        variables = {
            'B25001_001E': 'Total Housing Units',
            'B25003_002E': 'Owner Occupied Housing Units',
            'B25003_003E': 'Renter Occupied Housing Units',
            'B25064_001E': 'Median Gross Rent',
            'B19013_001E': 'Median Household Income',
            'B25077_001E': 'Median Home Value',
            'B08303_001E': 'Total Commuters',
            'B25024_002E': 'Single Family Detached',
            'B25024_003E': 'Single Family Attached',
            'B25024_004E': 'Buildings 2 Units',
            'B25024_005E': 'Buildings 3-4 Units',
            'B25024_006E': 'Buildings 5-9 Units',
            'B25024_007E': 'Buildings 10-19 Units',
            'B25024_008E': 'Buildings 20-49 Units',
            'B25024_009E': 'Buildings 50+ Units'
        }
        
        var_string = ','.join(variables.keys())
        
        for metro_code, metro_name in metros.items():
            try:
                url = f"https://api.census.gov/data/2022/acs/acs5"
                params = {
                    'get': var_string,
                    'for': f'metropolitan statistical area/micropolitan statistical area:{metro_code}'
                }
                
                response = requests.get(url, params=params, timeout=15)
                if response.status_code == 200:
                    data = response.json()
                    if len(data) > 1:  # First row is headers
                        metro_data = {}
                        headers = data[0]
                        values = data[1]
                        
                        for i, var_code in enumerate(variables.keys()):
                            if i < len(values):
                                try:
                                    value = float(values[i]) if values[i] not in [None, '-', ''] else None
                                    metro_data[var_code] = {
                                        'name': variables[var_code],
                                        'value': value,
                                        'source': 'Census Bureau ACS 2022'
                                    }
                                except (ValueError, TypeError):
                                    metro_data[var_code] = {
                                        'name': variables[var_code],
                                        'value': None,
                                        'source': 'Census Bureau ACS 2022'
                                    }
                        
                        census_data[metro_code] = {
                            'metro_name': metro_name,
                            'data': metro_data
                        }
                        print(f"âœ… {metro_name}: Census data retrieved")
                    else:
                        print(f"âš ï¸  {metro_name}: No census data available")
                else:
                    print(f"âŒ {metro_name}: Census API call failed (status: {response.status_code})")
                
                time.sleep(0.2)  # Rate limiting
                
            except Exception as e:
                print(f"âŒ Error fetching census data for {metro_name}: {e}")
        
        return census_data
    
    def get_bls_data(self):
        """Fetch data from Bureau of Labor Statistics API"""
        print("\nğŸ’¼ Fetching BLS employment data...")
        bls_data = {}
        
        # Key employment series for major metros
        # Format: LAUMT + FIPS code + unemployment rate
        metro_series = {
            'LAUMT171698000000003': 'Chicago-Naperville-Elgin Unemployment Rate',
            'LAUMT273346000000003': 'Minneapolis-St. Paul Unemployment Rate',
            'LAUMT251446000000003': 'Boston-Cambridge-Newton Unemployment Rate',
            'LAUMT123310000000003': 'Miami-Fort Lauderdale Unemployment Rate'
        }
        
        # National employment series
        national_series = {
            'CES0000000001': 'Total Nonfarm Employment',
            'CES2000000001': 'Construction Employment',
            'CES5553000001': 'Real Estate Employment',
            'CES6562000001': 'Professional Services Employment'
        }
        
        all_series = {**metro_series, **national_series}
        
        for series_id, series_name in all_series.items():
            try:
                # BLS API v2 - no API key required for public data
                url = "https://api.bls.gov/publicAPI/v2/timeseries/data/"
                
                # Get last 12 months of data
                current_year = datetime.now().year
                last_year = current_year - 1
                
                payload = {
                    "seriesid": [series_id],
                    "startyear": str(last_year),
                    "endyear": str(current_year),
                    "catalog": False,
                    "calculations": False,
                    "annualaverage": False
                }
                
                response = requests.post(url, json=payload, timeout=15)
                if response.status_code == 200:
                    data = response.json()
                    if data['status'] == 'REQUEST_SUCCEEDED' and data['Results']['series']:
                        series_data = data['Results']['series'][0]['data']
                        if series_data:
                            # Get most recent data point
                            latest = series_data[0]
                            bls_data[series_id] = {
                                'name': series_name,
                                'value': latest['value'],
                                'date': f"{latest['year']}-{latest['period'][1:]}",
                                'source': 'Bureau of Labor Statistics'
                            }
                            print(f"âœ… {series_name}: {latest['value']} (as of {latest['year']}-{latest['period'][1:]})")
                        else:
                            print(f"âš ï¸  {series_name}: No data available")
                    else:
                        print(f"âŒ {series_name}: BLS API request failed")
                else:
                    print(f"âŒ {series_name}: BLS API call failed (status: {response.status_code})")
                
                time.sleep(0.5)  # BLS rate limiting
                
            except Exception as e:
                print(f"âŒ Error fetching BLS data for {series_name}: {e}")
        
        return bls_data
    
    def get_zillow_research_data(self):
        """
        Fetch Zillow Research Data (CSV downloads)
        Note: This would typically involve downloading CSV files and parsing them
        For now, we'll simulate with recent data structure
        """
        print("\nğŸ  Processing Zillow Research Data...")
        
        # In a real implementation, you would:
        # 1. Download CSV files from https://www.zillow.com/research/data/
        # 2. Parse the CSV files for recent data
        # 3. Extract relevant metro area data
        
        # For demonstration, we'll create a structure for key Zillow metrics
        zillow_data = {
            'ZHVI': {  # Zillow Home Value Index
                'name': 'Zillow Home Value Index',
                'source': 'Zillow Research',
                'metros': {
                    'Chicago': {'value': 285000, 'change_yoy': 2.1},
                    'Minneapolis': {'value': 315000, 'change_yoy': 3.2},
                    'Boston': {'value': 685000, 'change_yoy': 1.8},
                    'Miami': {'value': 485000, 'change_yoy': 4.5}
                }
            },
            'ZRI': {  # Zillow Rent Index
                'name': 'Zillow Rent Index',
                'source': 'Zillow Research',
                'metros': {
                    'Chicago': {'value': 1850, 'change_yoy': 3.1},
                    'Minneapolis': {'value': 1650, 'change_yoy': 4.2},
                    'Boston': {'value': 2950, 'change_yoy': 2.8},
                    'Miami': {'value': 2450, 'change_yoy': 5.1}
                }
            }
        }
        
        print("âœ… Zillow Research Data structure prepared")
        print("ğŸ’¡ Note: In production, this would parse actual CSV downloads from Zillow")
        
        return zillow_data
    
    def fetch_all_data(self):
        """Fetch data from all sources"""
        print("ğŸš€ Comprehensive Data Collection Starting...")
        print("=" * 60)
        
        all_data = {
            'timestamp': datetime.now().isoformat(),
            'sources': {}
        }
        
        # FRED Economic Data
        fred_series = [
            ('FEDFUNDS', 'Federal Funds Rate'),
            ('GS10', '10-Year Treasury Rate'),
            ('GS5', '5-Year Treasury Rate'),
            ('GS2', '2-Year Treasury Rate'),
            ('UNRATE', 'National Unemployment Rate'),
            ('CPIAUCSL', 'Consumer Price Index'),
            ('GDPC1', 'Real GDP'),
            ('HOUST', 'Housing Starts'),
            ('PERMIT', 'Building Permits'),
            ('MORTGAGE30US', '30-Year Fixed Mortgage Rate'),
            ('CSUSHPISA', 'Case-Shiller Home Price Index'),
            ('RHORUSQ156N', 'Homeownership Rate')
        ]
        
        all_data['sources']['fred'] = self.get_fred_data(fred_series)
        all_data['sources']['census'] = self.get_census_data()
        all_data['sources']['bls'] = self.get_bls_data()
        all_data['sources']['zillow'] = self.get_zillow_research_data()
        
        # Save comprehensive data
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"comprehensive_market_data_{timestamp}.json"
        
        with open(filename, 'w') as f:
            json.dump(all_data, f, indent=2, default=str)
        
        print(f"\nğŸ’¾ Comprehensive data saved to: {filename}")
        print("ğŸ‰ Data collection complete!")
        
        return all_data

def main():
    """Main function to test comprehensive data fetching"""
    fetcher = ComprehensiveDataFetcher()
    data = fetcher.fetch_all_data()
    
    # Print summary
    print("\nğŸ“‹ Data Collection Summary:")
    print("=" * 40)
    
    for source, source_data in data['sources'].items():
        if isinstance(source_data, dict):
            count = len(source_data)
            print(f"{source.upper()}: {count} data points collected")
        else:
            print(f"{source.upper()}: Data structure prepared")

if __name__ == "__main__":
    main()
